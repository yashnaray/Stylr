#!/usr/bin/env python3
#
#   Our data comes from:
#
#       https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset
#
#   ...which in turn was generated by scraping `www.myntra.com`.
#
# IMAGES
#
#   Unless we're going to use some sort of visual machine-learning algorithm,
#   it's probably best just to use the images hosted on the Myntra servers.
#   These images are served with `Access-Control-Allow-Origin: *`,
#   so we can just point the browser directly to that site and
#   skip having to deal with the images at all on our end.
#   The URLs all seem to follow this format:
#
#       http://assets.myntassets.com/...
#
#   It should be noted that it's possible to get a different resolution by
#   stripping off an initial `v1/` from the pathname (if it exists) and
#   prepending `h_{height},q_{quality_percentage},w_{width}/v1/`,
#   where items in brackets are adjustable parameters.
#
# FILES
#
#   This script downloads the data required by the application and
#   performs some minimal data processing.
#
#   These files contain raw data:
#
#     - data/styles.csv: list of clothing articles and metadata
#     - data/images.csv: mapping between filenames and image URLs
#
#   These files are generated as output:
#
#     - data/items.csv: data in compact form
#     - enums.py: mappings from criteria numbers to names
#

import os
import sys

source_url = "https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset/fashion-dataset%2f"
source_files = ["styles.csv", "images.csv"]
database_file = "local.db"

gender_map = {
    b"Unisex": 0,
    b"Women": 1,
    b"Men": 2,
    b"Girls": None,
    b"Boys": None,
}

def assert_removeprefix(s, prefix):
    assert s.startswith(prefix)
    return s[len(prefix):]

def assert_removesuffix(s, suffix):
    assert s.endswith(suffix)
    return s[:-len(suffix)]

def scoreboard(d):
    return {key: i for i, key in enumerate(
        sorted(d, key=lambda key: (-d[key], key))
    )}

def dump_py_list(file, name, values):
    file.write(name + b" = [\n")
    for value in values:
        file.write(b"\t'" + value + b"',\n")
    file.write(b"]\n")

def dump_preferences(file, prefs, indent=b"\t"):
    for key, value in prefs.items():
        if value:
            file.write(indent + b"['" + key + b"', [\n")
            dump_preferences(file, value, indent + b"\t")
            file.write(indent + b"]],\n")
        else:
            file.write(indent + b"'" + key + b"',\n")

def download_source_file(file):
    from urllib.request import urlopen
    import io
    import shutil
    import zipfile
    url = source_url + file
    print(f"Downloading {url}")
    buffer = io.BytesIO()
    with urlopen(url) as response:
        shutil.copyfileobj(response, buffer)
    print(f"Extracting {file}")
    zipfile.ZipFile(buffer).extractall()

def init_data(*, download=False, force=False):
    for file in source_files:
        if download or not os.path.exists(file):
            download_source_file(file)

    if not force and os.path.exists("items"):
        return

    from collections import defaultdict
    def recursivedict():
        return defaultdict(recursivedict)

    images = {}
    items = []
    tagmap = defaultdict(lambda: len(tagmap) + 1)
    preferences = recursivedict()
    num_no_image = 0
    num_skipped = 0
    max_name = 0
    max_url = 0
    max_text = 0
    gender_freq = [0, 0, 0]

    with open("images.csv", "rb") as file:
        file.readline() # skip header
        for line in file:
            id, url = line.split(b",")
            id = assert_removesuffix(id, b".jpg")
            url = url.strip()
            if url == b"undefined":
                continue
            url = assert_removeprefix(url, b"http://assets.myntassets.com/")
            if url.startswith(b"v1/"):
                url = url[3:]
            images[id] = url

    with open("styles.csv", "rb") as file:
        file.readline() # skip header
        for line in file:
            id, gender, cat1, cat2, cat3, color, season, year, context, name = line.split(b",", 9)
            if id not in images:
                num_no_image += 1
                continue
            gender = gender_map[gender]
            if gender is None:
                num_skipped += 1
                continue
            gender_freq[gender] += 1
            preferences[b"Categories"][cat1][cat2][cat3] = None
            if color and color not in (b"Multi", b"NA", b"Unknown"):
                preferences[b"Colors"][color] = None
            if context and context not in (b"Multi", b"NA", b"Unknown"):
                preferences[b"Contexts"][context] = None
            tags = [
                tagmap[tag] if tag and tag not in (b"Multi", b"NA", b"Unknown") else 0
                for tag in [cat1, cat2, cat3, color, context]
            ]
            name = name.strip().replace(b",", b";")
            url = images[id]
            if len(name) > max_name:
                max_name = len(name)
            if len(url) > max_url:
                max_url = len(url)
            if len(name) + len(url) > max_text:
                max_text = len(name) + len(url)
            items.append((id, gender, tags, name, url))

    sys.stdout.write(
        f"items: {len(items)}\n"
        f"  unisex: {gender_freq[0]}\n"
        f"  female: {gender_freq[1]}\n"
        f"  male: {gender_freq[2]}\n"
        f"missing images: {num_no_image}\n"
        f"skipped: {num_skipped}\n"
        f"tags: {len(tagmap)}\n"
        f"max name: {max_name}\n"
        f"max url: {max_url}\n"
        f"max text: {max_text}\n"
    )

    with open("../enums.py", "wb") as file:
        file.write(b"# Automatically generated by conv.py\n")
        dump_py_list(file, b"gender_names", gender_map)
        dump_py_list(file, b"tag_names", tagmap)
        file.write(b"preferences = [\n")
        dump_preferences(file, preferences)
        file.write(b"]\n")

    with open("items", "wb") as file:
        for id, gender, tags, name, url in items:
            file.write(bytes([
                gender,
                *tags,
                len(name),
                len(url)
            ]) + name + url)

def init_database(*, reset=False):
    with open("../init.sql") as file:
        init_sql = file.read()

    if reset:
        try:
            os.remove(database_file)
        except FileNotFoundError:
            pass

    # Create tables and other structures
    import sqlite3
    conn = sqlite3.connect(database_file)
    try:
        cur = conn.cursor()
        cur.executescript(init_sql)
        conn.commit()
    finally:
        conn.close()

def main():
    import optparse
    parser = optparse.OptionParser()
    parser.add_option("-d", "--data", action="store_const", dest="target", const=1, help="initialize dataset only")
    parser.add_option("-b", "--database", action="store_const", dest="target", const=2, help="initialize database only")
    parser.add_option("-f", "--force", action="count", default=0, help="delete everything and start over (-ff to force download)")
    opts, args = parser.parse_args()
    if args:
        parser.error(f"unexpected argument {args[0]!r}")

    os.chdir(os.path.dirname(__file__))

    try:
        os.mkdir("data")
    except FileExistsError:
        pass

    os.chdir("data")

    if opts.target is None or opts.target == 1:
        init_data(download=opts.force >= 2, force=opts.force >= 1)
    if opts.target is None or opts.target == 2:
        init_database(reset=opts.force >= 1)

if __name__ == "__main__":
    main()
