#!/usr/bin/env python3
#
#   Our data comes from:
#
#       https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset
#
#   ...which in turn was generated by scraping `www.myntra.com`.
#
# IMAGES
#
#   Unless we're going to use some sort of visual machine-learning algorithm,
#   it's probably best just to use the images hosted on the Myntra servers.
#   These images are served with `Access-Control-Allow-Origin: *`,
#   so we can just point the browser directly to that site and
#   skip having to deal with the images at all on our end.
#   The URLs all seem to follow this format:
#
#       http://assets.myntassets.com/...
#
#   It should be noted that it's possible to get a different resolution by
#   stripping off an initial `v1/` from the pathname (if it exists) and
#   prepending `h_{height},q_{quality_percentage},w_{width}/v1/`,
#   where items in brackets are adjustable parameters.
#
# FILES
#
#   This script downloads the data required by the application and
#   performs some minimal data processing. All files are written into
#   the `data` directory.
#
#   These files contain raw data:
#
#     - styles.csv: list of clothing articles and metadata
#     - images.csv: mapping between filenames and image URLs
#
#   This script simply combines them into a single file:
#
#     - items.csv
#

from collections import defaultdict
import io
import optparse
import os
import shutil
import sys
import zipfile

base_url = "https://www.kaggle.com/api/v1/datasets/download/paramaggarwal/fashion-product-images-dataset/fashion-dataset%2f"
files = ["styles.csv", "images.csv"]

gender_map = {
    b"Unisex": 0,
    b"Women": 1,
    b"Men": 2,
    b"Girls": None,
    b"Boys": None,
}

def assert_removeprefix(s, prefix):
    assert s.startswith(prefix)
    return s[len(prefix):]

def assert_removesuffix(s, suffix):
    assert s.endswith(suffix)
    return s[:-len(suffix)]

def scoreboard(d):
    return {key: i for i, key in enumerate(
        sorted(d, key=lambda key: d[key], reverse=True)
    )}

def dump_py_list(file, name, values):
    file.write(name + b" = [\n")
    for value in values:
        file.write(b"  '" + value + b"',\n")
    file.write(b"]\n")

def main():
    parser = optparse.OptionParser()
    parser.add_option("-f", "--force", action="store_true", help="download files even if they exist")
    parser.add_option("-n", "--dry-run", action="store_true", help="don't write output files")
    opts, args = parser.parse_args()
    if args:
        parser.error(f"unexpected argument {args[0]!r}")

    os.chdir(os.path.dirname(__file__))

    try:
        os.mkdir("data")
    except FileExistsError:
        pass

    os.chdir("data")

    for file in files:
        if not opts.force and os.path.exists(file):
            continue
        from urllib.request import urlopen
        url = base_url + file
        print(f"Downloading {url}")
        buffer = io.BytesIO()
        with urlopen(url) as response:
            shutil.copyfileobj(response, buffer)
        print(f"Extracting {file}")
        zipfile.ZipFile(buffer).extract("data")

    images = {}
    items = []
    categories = defaultdict(int)
    colors = defaultdict(int)
    contexts = defaultdict(int)
    num_no_image = 0
    num_skipped = 0
    max_name = 0
    max_url = 0
    max_text = 0
    gender_freq = [0, 0, 0]

    with open("images.csv", "rb") as file:
        file.readline() # skip header
        for line in file:
            id, url = line.split(b",")
            id = assert_removesuffix(id, b".jpg")
            url = url.strip()
            if url == b"undefined":
                continue
            url = assert_removeprefix(url, b"http://assets.myntassets.com/")
            if url.startswith(b"v1/"):
                url = url[3:]
            images[id] = url

    with open("styles.csv", "rb") as file:
        file.readline() # skip header
        for line in file:
            id, gender, cat1, cat2, cat3, color, season, year, context, name = line.split(b",", 9)
            if id not in images:
                num_no_image += 1
                continue
            gender = gender_map[gender]
            if gender is None:
                num_skipped += 1
                continue
            gender_freq[gender] += 1
            cats = {cat1, cat2, cat3}
            for cat in cats:
                categories[cat] += 1
            if not color or color == b"NA":
                color = b"Unknown"
            colors[color] += 1
            if not context or context == b"NA":
                context = b"Unknown"
            contexts[context] += 1
            name = name.strip().replace(b",", b";")
            url = images[id]
            if len(name) > max_name:
                max_name = len(name)
            if len(url) > max_url:
                max_url = len(url)
            if len(name) + len(url) > max_text:
                max_text = len(name) + len(url)
            items.append((id, gender, cats, color, context, name, url))

    sys.stdout.write(
        f"items: {len(items)}\n"
        f"  unisex: {gender_freq[0]}\n"
        f"  female: {gender_freq[1]}\n"
        f"  male: {gender_freq[2]}\n"
        f"missing images: {num_no_image}\n"
        f"skipped: {num_skipped}\n"
        f"categories: {len(categories)}\n"
        f"colors: {len(colors)}\n"
        f"contexts: {len(contexts)}\n"
        f"max name: {max_name}\n"
        f"max url: {max_url}\n"
        f"max text: {max_text}\n"
    )

    if opts.dry_run:
        return

    categories = scoreboard(categories)
    colors = scoreboard(colors)
    contexts = scoreboard(contexts)

    with open("../enums.py", "wb") as file:
        file.write(b"# Automatically generated by conv.py\n")
        dump_py_list(file, b"gender_names", gender_map)
        dump_py_list(file, b"category_names", categories)
        dump_py_list(file, b"color_names", colors)
        dump_py_list(file, b"context_names", contexts)

    with open("items", "wb") as file:
        for id, gender, cats, color, context, name, url in items:
            cats = [categories[cat] for cat in cats]
            cats.sort()
            while len(cats) < 3:
                cats.append(0)
            file.write(bytes([
                gender,
                *cats,
                colors[color],
                contexts[context],
                len(name),
                len(url)
            ]) + name + url)

if __name__ == "__main__":
    main()
